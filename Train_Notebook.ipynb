{"cells":[{"cell_type":"markdown","metadata":{"id":"HEA35evOgLUL"},"source":["Basics of Deep Learning assignment - Train Notebook\n","\n","---\n","Written by:\n","\n","- Matan Ofri\n","- Itamar Kirsch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07Tn-gI9iJN4"},"outputs":[],"source":["import keras as ker\n","import numpy as np\n","import os\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from keras.models import Sequential\n","from keras.callbacks import EarlyStopping , ModelCheckpoint, ReduceLROnPlateau\n","from keras.optimizers import Adam, RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing import image\n","from keras.layers import Conv2D, MaxPooling2D, SeparableConv2D, Flatten, BatchNormalization, Dropout, Dense, UpSampling2D, Conv2DTranspose\n","from keras import initializers,layers, Input, backend, Model\n","from keras.initializers import glorot_normal\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","from sklearn.neighbors import KNeighborsClassifier\n","from tensorflow.keras.utils import plot_model"]},{"cell_type":"markdown","metadata":{"id":"zmNJtzkUyi89"},"source":["\n","\n","Load Data from Google Drive\n"]},{"cell_type":"markdown","source":["Please enter the path to the Basic folder"],"metadata":{"id":"-26eNKG4XdO6"}},{"cell_type":"code","source":["PATH='/content/drive/Shareddrives/DeepLearning/Basics/'"],"metadata":{"id":"uGbu8YBkXc4w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCSmgE0Z1PXn"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2652,"status":"ok","timestamp":1708550165833,"user":{"displayName":"Matan Ofri","userId":"12844360060930968709"},"user_tz":-120},"id":"TJt0vbClkMO_","outputId":"24d2a273-4740-48e5-d722-00fef85d13a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount('/content/drive')\n","#Section A - Binary Classification\n","val_binary = PATH+'Kaggle Data/chest_xray_2/val'\n","train_binary = PATH+'Kaggle Data/chest_xray_2/train'\n","#Section B - Multi-Class\n","val_multiclass =PATH+'Kaggle Data/chest_xray_3/val'\n","train_multiclass =PATH+'Kaggle Data/chest_xray_3/train'"]},{"cell_type":"markdown","metadata":{"id":"TaKhcX4Rp7o9"},"source":["Load data for local use on RTX4090 (GPU) with Docker:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8z6mA60pbdk"},"outputs":[],"source":["#Section A - Binary Classification\n","val_binary = '/chest_xray/val'\n","train_binary = '/chest_xray/train'\n","#Section B - Multi-Class\n","val_multiclass ='/chest_xray3/val'\n","train_multiclass ='/chest_xray3/train'\n","if os.path.exists(val_binary)& os.path.exists(val_multiclass):\n","  print(\"Val data is loaded\")\n","else:\n","  print(\"There is a problem loading validation data\")\n","if os.path.exists(train_binary) & os.path.exists(train_multiclass):\n","  print(\"Train data is loaded\")\n","else:\n","  print(\"There is a problem loading train data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7vutB-YVo0d"},"outputs":[],"source":["#Section A - Binary Classification\n","val_binary = '/root/chest_xray_2/val'\n","train_binary = '/root/chest_xray_2/train'\n","#Section B - Multi-Class\n","val_multiclass ='/root/chest_xray_3/val'\n","train_multiclass ='/root/chest_xray_3/train'\n","if os.path.exists(val_binary)& os.path.exists(val_multiclass):\n","  print(\"Val data is loaded\")\n","else:\n","  print(\"There is a problem loading validation data\")\n","if os.path.exists(train_binary) & os.path.exists(train_multiclass):\n","  print(\"Train data is loaded\")\n","else:\n","  print(\"There is a problem loading train data\")"]},{"cell_type":"markdown","metadata":{"id":"Kfvtt1xOiu5w"},"source":["# Question number 1, Section A - Binary Classification\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTuQ5H8JtN45"},"outputs":[],"source":["batch_size = 20\n","img_width = 256\n","img_height = 256\n","epochs = 50"]},{"cell_type":"markdown","metadata":{"id":"hZtHi7mD0Y7d"},"source":["1. Data Augmentation (Preprocessing Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Wa708ISquUD"},"outputs":[],"source":["binary_data_datagen = ImageDataGenerator(\n","   rescale=1./255,\n","   rotation_range=20,\n","   width_shift_range=0.2,\n","   height_shift_range=0.2,\n","   shear_range=0.2,\n","   zoom_range=0.2,\n","   horizontal_flip=True,\n","   vertical_flip=True,\n","   fill_mode='constant',\n","   cval = 0,\n",")\n","\n","binary_train_generator = binary_data_datagen.flow_from_directory(\n","    train_binary,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    seed = 26,\n","    #The samples are arranged in folders according to label, so we shuffle them:\n","    shuffle = True,\n","    class_mode='binary',\n","    color_mode='grayscale'\n",")\n","\n","binary_validation_generator = binary_data_datagen.flow_from_directory(\n","    val_binary,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    seed = 26,\n","    shuffle = True,\n","    class_mode='binary',\n","    color_mode='grayscale'\n",")"]},{"cell_type":"markdown","metadata":{"id":"gWmBnwpITpjr"},"source":["2. Architecture - Binary Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSk_ylU0VS8k"},"outputs":[],"source":["model = ker.Sequential()\n","#32 Inputs Block:\n","model.add(Conv2D(32, 3, activation='relu', strides=2, padding = 'same', name='Conv2D_input', input_shape=(img_height, img_width, 1),kernel_initializer=glorot_normal()))\n","model.add(MaxPooling2D(2, name='MaxPooling2D_input'))\n","#64-A Inputs Block:\n","model.add(Conv2D(64, 3, activation='relu', strides=2, padding = 'same', name='Conv2D_1'))\n","model.add(MaxPooling2D(2, name='MaxPooling2D_1'))\n","#128 Inputs Block:\n","model.add(Conv2D(128, 3, activation='relu', strides=2, padding = 'same', name='Conv2D_2'))\n","model.add(MaxPooling2D(2, name='MaxPooling2D_2'))\n","#64-B Inputs Block:\n","model.add(Conv2D(64, 3, activation='relu', strides=2, padding = 'same', name='Conv2D_3'))\n","model.add(MaxPooling2D(2, name='MaxPooling2D_3'))\n","\n","model.add(Flatten(name='Flatten'))\n","model.add(Dense(64, activation='relu', name='Dense_1'))\n","#Binary classification, so we use 1 unit and 'sigmoid' activation:\n","model.add(Dense(1, activation='sigmoid', name='Dense_output'))\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"5j3-bkMjz_B_"},"source":["3. The model training function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9eEKsFeD7GDR"},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=3,  restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss',mode='auto', factor=0.5, patience=2, verbose=1)\n","\n","history = model.fit(\n","    binary_train_generator,\n","    steps_per_epoch = binary_train_generator.samples//batch_size,\n","    epochs = epochs,\n","    validation_data = binary_validation_generator,\n","    validation_steps = binary_validation_generator.samples//batch_size,\n","    callbacks=[early_stopping,reduce_lr],\n","    verbose=2\n",")\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs_n = range(1, len(train_loss) + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V-TIVeHcE45"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(12, 6))\n","plt.plot(epochs_n, train_loss, 'bo-', label='Training loss')\n","plt.plot(epochs_n, val_loss, 'ro-', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eyK6kvTHE3zx"},"source":["# Question number 1, Section B - Multi-Class Classification\n","\n","---\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK8gHLapSHqr"},"outputs":[],"source":["batch_size = 32\n","img_width = 96\n","img_height = 96\n","epochs = 50"]},{"cell_type":"markdown","metadata":{"id":"LEHxnbNr0vOL"},"source":["1. Data Augmentation (Preprocessing Data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7I16AdUHd0R"},"outputs":[],"source":["multiclass_data_datagen = ImageDataGenerator(\n","     rescale=1./255,\n","     rotation_range=30,\n","     width_shift_range=0.1,\n","     height_shift_range=0.1,\n","     zoom_range=0.2,\n","     horizontal_flip=False,\n","     vertical_flip=False,\n",")\n","\n","multiclass_train_generator = multiclass_data_datagen.flow_from_directory(\n","    train_multiclass,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    shuffle = True,\n","    #Change for multiclass:\n","    class_mode='categorical',\n",")\n","\n","validation_generator = multiclass_data_datagen.flow_from_directory(\n","    val_multiclass,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    shuffle = True,\n","    #Change for multiclass:\n","    class_mode='categorical',\n",")"]},{"cell_type":"markdown","metadata":{"id":"rkH-h2JD0wO2"},"source":["2. Creating initial weights according to the amount of samples in every class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmXT1ms-kBlO"},"outputs":[],"source":["print(multiclass_train_generator.class_indices)\n","temp = Counter(multiclass_train_generator.classes)\n","sum = temp[0]+ temp[1]+ temp[2]\n","BACTERIA_WEIGHT=((1/temp[0])*(sum/3.0))\n","NORMAL_WEIGHT=((1/temp[1])*(sum/3.0))\n","VIRUS_WEIGHT=((1/temp[2])*(sum/3.0))\n","ALL_WEIGHT={0: BACTERIA_WEIGHT,1:NORMAL_WEIGHT,2:VIRUS_WEIGHT}"]},{"cell_type":"markdown","metadata":{"id":"NszX1A3K1FZ0"},"source":["3. Architecture - Multi-Class Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcAKxqmqqqIZ"},"outputs":[],"source":["model = Sequential()\n","model.add(Conv2D(32, (3,3), activation = 'relu', name='Conv2D_input',input_shape = (img_height, img_width, 3)))\n","model.add(MaxPooling2D((2,2), name='MaxPooling2D_input'))\n","\n","model.add(Conv2D(64, (3,3), activation = 'relu', name='Conv2D_1'))\n","model.add(MaxPooling2D((2,2), name='MaxPooling2D_1'))\n","model.add(Dropout(0.2, name='Dropout_1'))\n","\n","model.add(Conv2D(128, (3,3), activation = 'relu', name='Conv2D_2'))\n","model.add(MaxPooling2D((2,2), name='MaxPooling2D_2'))\n","model.add(Dropout(0.2, name='Dropout_2'))\n","\n","model.add(Conv2D(256 , (3,3), activation = 'relu', name='Conv2D_3'))\n","model.add(MaxPooling2D((2,2), name='MaxPooling2D_3'))\n","\n","model.add(Flatten(name='Flaten'))\n","model.add(Dropout(0.2, name='Dropuot_flatten'))\n","model.add(Dense(256 , activation = 'relu', name='Dense_flatten'))\n","model.add(Dense(3, activation='softmax', name='Dense_output'))\n","model.compile(optimizer = RMSprop(learning_rate=0.001, rho=0.90, epsilon=1e-08),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"o7ziedb31OUM"},"source":["4. The model training function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0w1ZnE76GNXJ"},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=7,  restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss',mode='auto', factor=0.5, patience=2, verbose=1)\n","\n","history = model.fit(\n","    multiclass_train_generator,\n","    steps_per_epoch = multiclass_train_generator.samples//batch_size,\n","    epochs = epochs,\n","    validation_data = validation_generator,\n","    validation_steps = validation_generator.samples//batch_size,\n","    class_weight=ALL_WEIGHT,\n","    callbacks=[early_stopping,reduce_lr],\n","    verbose=2\n",")\n","\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","epochs_n = range(1, len(train_loss) + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8T031ZbS8zE"},"outputs":[],"source":["train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","train_acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","epochs_n = range(2, len(train_loss) + 1)\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(12, 6))\n","train_loss.pop(0)\n","val_loss.pop(0)\n","plt.plot(epochs_n, train_loss, 'bo-', label='Training loss')\n","plt.plot(epochs_n, val_loss, 'ro-', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TuvT5r_NjmkT"},"source":["5. Save Embedding Vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w11AXOcyi-Wj"},"outputs":[],"source":["#Save Embedding Vector from H5 file:\n","from keras.models import model_from_json\n","model_json = embedding_model.to_json()\n","with open(\"embedding_model.json\", \"w\") as json_file:\n","  json_file.write(model_json)\n","embedding_model.save_weights(\"embedding_multicalss_vector.h5\")"]},{"cell_type":"markdown","metadata":{"id":"qQ9KBZLLkwZS"},"source":["# Question number 2 - KNN with Embedding Vector\n","\n","---\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icOQ8N6bXtlH"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","import pickle\n","from sklearn.manifold import TSNE\n","from mpl_toolkits.mplot3d import Axes3D\n","from keras.models import load_model\n","from keras.models import model_from_json\n","from joblib import dump ,load\n","import random"]},{"cell_type":"markdown","metadata":{"id":"zlzFV3mNcmOG"},"source":["1. Load our Classification Model and Create the Embedding Model"]},{"cell_type":"markdown","metadata":{"id":"YbrD6XyF1Hv_"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjp0Loi2jRuH"},"outputs":[],"source":["model = load_model(PATH+'Models/KNN.keras')\n","embedding_model = Sequential()\n","for layer in model.layers[:-1]:\n"," embedding_model.add(layer)\n","embedding_model.compile()"]},{"cell_type":"markdown","metadata":{"id":"PILzEgFIc_dF"},"source":["2. Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RccPLpBThiLS"},"outputs":[],"source":["img_height = 300\n","img_width = 300\n","batch_size=20\n","\n","datagen = ImageDataGenerator(rescale=1./255)\n","\n","validation_set_knn = datagen.flow_from_directory(\n","    val_multiclass,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical',\n","    color_mode='grayscale',\n","    shuffle = False\n",")\n","train_set_knn = datagen.flow_from_directory(\n","    train_multiclass,\n","    target_size = (img_height, img_width),\n","    class_mode='categorical',\n","    color_mode='grayscale',\n","    shuffle=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"hAw_d7-xJblN"},"source":["- Note - Creating the embedded vectors on the Google Colab takes over 30 minutes\n","- For testing purposes we will load the embedded predictions in the next block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rXv8HHpilp-"},"outputs":[],"source":["  #embeddings1 = embedding_model.predict(train_set_knn)\n","  #embeddings2 = embedding_model.predict(validation_set_knn)\n","  #allembeddings=[]\n","  #allembeddings = np.concatenate([embeddings1, embeddings2], axis=0)\n","  #alllabels = np.concatenate([train_set_knn.classes, validation_set_knn.classes], axis=0)"]},{"cell_type":"markdown","metadata":{"id":"X8N5u4e80nkZ"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xADkI0o1nRD"},"outputs":[],"source":["allembeddings= np.load(PATH+'Embeddings/allembeddings.npy')\n","alllabels= np.load(PATH+'Embeddings/alllabels.npy')"]},{"cell_type":"markdown","metadata":{"id":"CGbR_J0-JrOR"},"source":["3. Construction of the KNN model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NhmxGuqQYe4x"},"outputs":[],"source":["knn_classifier = KNeighborsClassifier(n_neighbors=72,weights='distance',metric='hamming')\n","knn_classifier.fit(allembeddings, alllabels)"]},{"cell_type":"markdown","source":["4. Save KNN Model for test notebook"],"metadata":{"id":"FTSNfbwlPm2a"}},{"cell_type":"code","source":["knnFile = open(PATH+'Models/knnpickle_file', 'wb')\n","pickle.dump(knn_classifier, knnFile)\n","knnFile.close()"],"metadata":{"id":"1MvFOCv7M0zJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zM6sDvNaPH9"},"source":["5. Create a 3D graph with t-SNE library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fGrvpcRsYmKY"},"outputs":[],"source":["class_names = {\n","    0: 'Bacteria',\n","    1: 'Normal',\n","    2: 'Virus'\n","}\n","\n","# Reduce dimensionality using t-SNE\n","tsne = TSNE(n_components=3)\n","allembeddings_tsne = tsne.fit_transform(allembeddings)\n","\n","# scaling factors for each component\n","scale_factors = [2.0, 1.5, 1.0]\n","\n","# Scale t-SNE components\n","allembeddings_tsne_scaled = allembeddings_tsne * scale_factors\n","\n","# Plot the data points in 3D\n","fig = plt.figure(figsize=(30, 30))\n","\n","ax = fig.add_subplot(111, projection='3d')\n","for label in np.unique(alllabels):\n","    ax.scatter(allembeddings_tsne_scaled[alllabels == label, 0],\n","               allembeddings_tsne_scaled[alllabels == label, 1],\n","               allembeddings_tsne_scaled[alllabels == label, 2],\n","               label=class_names[label], s=20, alpha=0.5)\n","\n","ax.view_init(elev=30, azim=45)\n","ax.set_title('t-SNE Visualization of X-ray Scans in 3D: Dividing Images into 3 Classes')\n","ax.legend(fontsize=20)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3mQg9zZ-E89"},"outputs":[],"source":["fig = plt.figure(figsize=(30, 30))\n","ax = fig.add_subplot(111, projection='3d')\n","for label in np.unique(alllabels):\n","    ax.scatter(allembeddings_tsne_scaled[alllabels == label, 0],\n","               allembeddings_tsne_scaled[alllabels == label, 1],\n","               allembeddings_tsne_scaled[alllabels == label, 2],\n","               label=class_names[label], s=20, alpha=0.5)\n","\n","ax.view_init(elev=30, azim=45)\n","ax.set_title('t-SNE Visualization of X-ray Scans in 3D: Dividing Images into 3 Classes')\n","ax.legend(fontsize=20)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iOOSygwSd1SS"},"source":["# Question number 3 - Identifying anomalies in X-ray images\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"AgVVhPqIxgYU"},"source":["1. Load data for anomaly detection model"]},{"cell_type":"markdown","metadata":{"id":"GRWaSTa-0cjH"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFfgvFeld-_i"},"outputs":[],"source":["drive.mount('/content/drive')\n","train_dir = PATH+'Kaggle Data/anomaly/train'\n","val_dir = PATH+'Kaggle Data/anomaly/val'\n","anom_dir = PATH+'Kaggle Data/anomaly/anom'\n","if os.path.exists(train_dir)&os.path.exists(anom_dir)& os.path.exists(val_dir):\n","  print(\"Data is loaded\")\n","else:\n","  print(\"There is a problem loading data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arN_BnZYjLTw"},"outputs":[],"source":["batch_size = 20\n","img_height = 384\n","img_width = 384\n","latent_dim = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olDs_51djMWg"},"outputs":[],"source":["train_datagen = ImageDataGenerator(\n","    rescale=1./255\n",")\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='input',\n","\n",")\n","\n","validation_generator = train_datagen.flow_from_directory(\n","    val_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='input',\n","\n","\n",")\n","\n","anomaly_generator = train_datagen.flow_from_directory(\n","   anom_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='input',\n","\n",")"]},{"cell_type":"markdown","metadata":{"id":"4nCjajmpxr7d"},"source":["2. Encoder Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KB2ZtCcQjUO6"},"outputs":[],"source":["# There is no need to run this section - you can load a ready-made model below\n","input_shape = (img_height, img_width, 1)\n","inputs = Input(shape=input_shape,name='Input')\n","\n","x = inputs\n","x = Conv2D(8, 1, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D1')(x)\n","x = Conv2D(16, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D2')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling1')(x)\n","x = Conv2D(32, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D3')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling2')(x)\n","x = Conv2D(64, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D4')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling3')(x)\n","x = Conv2D(128, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D5')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling4')(x)\n","x = Conv2D(256, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D6')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling5')(x)\n","x = Conv2D(512, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2D7')(x)\n","x = MaxPooling2D(2, padding = 'same',name='MaxPooling6')(x)\n","shape = backend.int_shape(x)\n","x = Flatten(name='Flatten')(x)\n","latent = Dense(latent_dim,name='output')(x)\n","encoder = Model(inputs,latent)\n","encoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"hLTN4Lr1xxJj"},"source":["3. Decoder Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAaUiup3jVaB"},"outputs":[],"source":["# There is no need to run this section - you can load a ready-made model below\n","latent_inputs = Input(shape=(latent_dim,),name='input')\n","x = Dense(shape[1]*shape[2]*shape[3],name=\"Dense\")(latent_inputs)\n","x = ker.layers.Reshape((shape[1],shape[2],shape[3]),name='Reshape')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D1')(x)\n","x = Conv2DTranspose(512, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose1')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D2')(x)\n","x = Conv2DTranspose(256, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose2')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D3')(x)\n","x = Conv2DTranspose(128, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose3')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D4')(x)\n","x = Conv2DTranspose(64, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose4')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D5')(x)\n","x = Conv2DTranspose(32, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose5')(x)\n","x = UpSampling2D((2,2),name='Upsampling2D6')(x)\n","x = Conv2DTranspose(16, 3, activation=LeakyReLU(alpha=0.3), padding = 'same',name='Conv2DTranspose6')(x)\n","x = Conv2DTranspose(8, 1, activation=backend.tanh, padding = 'same',name='Conv2DTranspose7')(x)\n","output = Conv2DTranspose(1, (1,1), activation='sigmoid', padding = 'same',name='Output')(x)\n","decoder = Model(latent_inputs, output)\n","decoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yp-FWjNY23HV"},"outputs":[],"source":["plot_model(decoder, to_file='encoder.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"kU97ftnLx0hC"},"source":["4. Autoencoder model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3MHtNk6lkV_"},"outputs":[],"source":["# There is no need to run this section - you can load a ready-made model below\n","autoencoder = Model(inputs,decoder(encoder(inputs)))\n","autoencoder.compile(optimizer=Adam(learning_rate=0.0005, beta_1=0.5, beta_2=0.9), loss='mse', metrics='accuracy',run_eagerly=True)#We should try SSIM loss\n","autoencoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"Mt0sczTGx4Ha"},"source":["5. Loading a pre-trained model"]},{"cell_type":"markdown","metadata":{"id":"rC4aAPLb0Yvp"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsXeGyGuloRS"},"outputs":[],"source":["autoencoder=ker.models.load_model(PATH+'Models/autoencoder.keras')\n","autoencoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"SVsRI5ZKyb2F"},"source":["6. Viewing X-ray images of lungs alongside images reproduced by the decoder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tlC49XgKuXVU"},"outputs":[],"source":["data_list = []\n","batch_index = 0\n","while batch_index <= train_generator.batch_index:\n","    data = train_generator.next()\n","    data_list.append(data[0])\n","    batch_index = batch_index + 1\n","predicted = autoencoder.predict(train_generator)\n","no_of_samples = 4\n","_, axs = plt.subplots(no_of_samples, 3, figsize=(5, 8))\n","axs = axs.flatten()\n","imgs = []\n","for i in range(no_of_samples):\n","    imgs.append(data_list[0][i])\n","    imgs.append(predicted[i])\n","    imgs.append(np.abs(predicted[i]-data_list[0][i]))\n","for img, ax in zip(imgs, axs):\n","    ax.imshow(img,cmap='magma')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EZoGRj2gy-Ao"},"source":["7. Creating embeddings vectors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilo0qAHUnEym"},"outputs":[],"source":["# There is no need to run this section - you can load a ready-made vectors below\n","prediction_loss2=[]\n","prediction_loss1=[]\n","prediction_loss3=[]\n","\n","\n","\n","\n","image_files = [os.path.join(train_dir+'/NORMAL', file) for file in os.listdir(train_dir+'/NORMAL') if file.endswith('.jpeg')]\n","\n","for dirs in image_files:\n","  img = image.load_img(dirs, target_size=(img_height, img_width))\n","  gray_img = img.convert('L')\n","  img_array = image.img_to_array(gray_img)\n","  img_array = np.expand_dims(img_array, axis=0)\n","  img_array = img_array / 255.0\n","  reconstructed_image = autoencoder.predict(img_array)\n","  prediction_loss1.append(np.mean(np.abs((reconstructed_image -img_array))))\n","\n","\n","image_files = [os.path.join(val_dir+'/NORMAL', file) for file in os.listdir('/content/anomaly/Normal_val/NORMAL') if file.endswith('.jpeg')]\n","\n","for dirs in image_files:\n","  img = image.load_img(dirs, target_size=(img_height, img_width))\n","  gray_img = img.convert('L')\n","  img_array = image.img_to_array(gray_img)\n","  img_array = np.expand_dims(img_array, axis=0)\n","  img_array = img_array / 255.0\n","  reconstructed_image = autoencoder.predict(img_array)\n","  prediction_loss3.append(np.mean(np.abs((reconstructed_image -img_array))))\n","\n","\n","selected_class = 'Sick'\n","image_files = [os.path.join(anom_dir+'/Sick', file) for file in os.listdir(anom_dir+'/Sick') if file.endswith('.jpeg')]\n","\n","for dirs in image_files:\n","  img = image.load_img(dirs, target_size=(img_height, img_width))\n","  gray_img = img.convert('L')\n","  img_array = image.img_to_array(gray_img)\n","  img_array = np.expand_dims(img_array, axis=0)\n","  img_array = img_array / 255.0\n","  reconstructed_image = autoencoder.predict(img_array)\n","\n","  prediction_loss2.append(np.mean(np.abs((reconstructed_image -img_array))))\n","\n","# Flatten the ndarray to create a 1D list of values and append the flattened values to a larger list:\n","test_reconstruction_values = []\n","anom_reconstruction_values = []\n","val_reconstruction_values = []\n","\n","for ndarray in prediction_loss1:\n","    flattened_values = ndarray.flatten()\n","    test_reconstruction_values.extend(flattened_values)\n","\n","for ndarray in prediction_loss2:\n","    flattened_values = ndarray.flatten()\n","    anom_reconstruction_values.extend(flattened_values)\n","for ndarray in prediction_loss3:\n","    flattened_values = ndarray.flatten()\n","    val_reconstruction_values.extend(flattened_values)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OPzCn1ZT0NWV"},"source":["ðŸ›‘**Please note - you must load this block:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWaRhdJZnMRZ"},"outputs":[],"source":["train_and_val_flattened_values= np.load(PATH+'Embeddings/train_and_val_flattened_values.npy')\n","anomaly_flattened_values= np.load(PATH+'Embeddings/anomaly_flattened_values.npy')"]},{"cell_type":"markdown","metadata":{"id":"uhOBALNuzVpu"},"source":["8. A graph showing the loss relative to the amount of samples for which the value was obtained\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"50IRGBKPoJjh"},"outputs":[],"source":["loss_threshold_test=  np.percentile(test_reconstruction_values, 98)\n","print(f'The prediction loss threshold train for 2% of the outliers is {loss_threshold_test}')\n","loss_threshold_anom= np.percentile(anom_reconstruction_values, 98)\n","print(f'The prediction loss threshold anom for 2% of the outliers is {loss_threshold_anom}')\n","\n","loss_threshold_val= np.percentile(val_reconstruction_values, 98)\n","print(f'The prediction loss threshold val for 2% of the outliers is {loss_threshold_val}')\n","flat1=np.array(test_reconstruction_values).reshape(-1,1)\n","flat2=np.array(anom_reconstruction_values).reshape(-1,1)\n","flat3=np.array(val_reconstruction_values).reshape(-1,1)\n","plt.hist(flat1,alpha=0.5,color='green')\n","plt.hist(flat2,alpha=0.5,color='red')\n","plt.hist(flat3,alpha=0.5,color='yellow')\n","plt.axvline(x=loss_threshold_test,color= 'green')\n","plt.axvline(x=loss_threshold_anom,color= 'red')\n","plt.axvline(x=loss_threshold_val,color= 'yellow')\n","plt.xlabel('Prediction Loss')\n","plt.ylabel('Frequency')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["Kfvtt1xOiu5w","eyK6kvTHE3zx","qQ9KBZLLkwZS","iOOSygwSd1SS"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}